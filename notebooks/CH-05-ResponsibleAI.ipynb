{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3964eae",
   "metadata": {},
   "source": [
    "# Reto 5: IA Responsable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "132f6f55",
   "metadata": {},
   "source": [
    "A medida que los LLMs crecen en popularidad y uso en todo el mundo, la necesidad de gestionar y monitorear sus salidas se vuelve cada vez más importante. En este desafío, aprenderás cómo evaluar las salidas de los LLMs y cómo identificar y mitigar posibles sesgos en el modelo.\n",
    "\n",
    "Preguntas que deberías poder responder al final de este desafío:\n",
    "- ¿Cómo puedes aprovechar el filtrado de contenido?\n",
    "- ¿Cuáles son las formas de evaluar la veracidad y reducir las alucinaciones?\n",
    "- ¿Cómo puedes identificar y mitigar el sesgo en tu modelo?\n",
    "\n",
    "Secciones en este Desafío:\n",
    "\n",
    "1. Identificar daños y detectar Información Personal Identificable (PII)<!--(#content-filtering,-content-safety,-and-personal-identifiable-information-(pii)-detection)-->\n",
    "1. Evaluar la veracidad utilizando Conjuntos de Datos de Verdad Fundamental (Ground Truth Datasets)<!--(#evaluating-truthfulness-using-ground-truth-data)-->\n",
    "1. Evaluar la veracidad utilizando GPT sin Conjuntos de Datos de Verdad Fundamental<!--(#evaluating-truthfulness-using-gpt-without-ground-truth-datasets)-->\n",
    "\n",
    "Recursos:\n",
    "- [Descripción general de las prácticas de IA Responsable para modelos de Azure OpenAI](https://learn.microsoft.com/en-us/legal/cognitive-services/openai/overview)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1fdf2ed6",
   "metadata": {},
   "source": [
    "## 1. Filtrado de Contenido, Content Safety y Detección de Información Personal Identificable (PII)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e860826",
   "metadata": {},
   "source": [
    "Las cuatro etapas de las recomendaciones de IA Responsable al usar OpenAI son identificar, medir, mitigar y operar daños. En esta sección, nos centraremos en identificar daños.\n",
    "\n",
    "Este paso tiene como objetivo identificar posibles daños para que puedas mitigarlos eficazmente. Es importante recordar que identificar daños depende en gran medida del contexto. Por ejemplo, un modelo que se utiliza para generar texto para un libro infantil tendrá diferentes daños que un modelo que se utiliza para generar texto para un artículo de noticias. El lenguaje también tendrá diferentes significados en diferentes contextos, por lo que un marco de identificación debe ser lo suficientemente flexible como para adaptarse a varias situaciones.\n",
    "\n",
    "Presentamos tres herramientas para identificar daños:\n",
    "- Azure AI Services Content Filtering (Filtrado de Contenido)\n",
    "- Azure AI Content safety\n",
    "- Detección de PII a través de Plugins de OpenAI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b5a13fe6",
   "metadata": {},
   "source": [
    "### 1.1 Azure AI Services Content Filtering\n",
    "\n",
    "De acuerdo a la [documentación de Azure](https://learn.microsoft.com/es-mx/azure/cognitive-services/openai/concepts/content-filter): \n",
    "\n",
    "    El Servicio Azure OpenAI incluye un sistema de gestión de contenido que trabaja junto a los modelos principales para filtrar contenido. Este sistema funciona ejecutando tanto el prompt de entrada como el contenido generado a través de un conjunto de modelos de clasificación destinados a detectar el uso indebido.\n",
    "\n",
    "Debes evaluar cuidadosamente todos los posibles daños y agregar mitigaciones específicas del escenario según sea necesario. Por ejemplo, es posible que desees filtrar contenido que sea ofensivo, profano, sexualmente explícito o lleno de odio.\n",
    "\n",
    "**Verificación de Conocimientos #1:**:\n",
    "\n",
    "Para evaluar tu comprensión del concepto de filtrado de contenido, responde las siguientes preguntas basándote en la documentación:\n",
    "\n",
    "* Verdadero o Falso: Si realizas una solicitud de completado en streaming para múltiples respuestas, el filtro de contenido evaluará cada respuesta individualmente y devolverá solo las que pasen.\n",
    "* Verdadero o Falso: el parámetro `finish_reason` se devolverá en cada respuesta del filtro de contenido.\n",
    "* Verdadero o Falso: Si el sistema de filtrado de contenido no está disponible, no podrás recibir resultados sobre tu solicitud."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67c23d99",
   "metadata": {},
   "source": [
    "### 1.2 Azure AI Content Safety"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9e0d7a6",
   "metadata": {},
   "source": [
    "[Azure AI Content Safety](https://learn.microsoft.com/es-mx/azure/cognitive-services/content-safety/overview) fue creado para ayudar a las organizaciones a gestionar y moderar de manera responsable el contenido generado por usuarios y por la IA. Es un servicio administrado que proporciona una solución de moderación de contenido escalable, de baja latencia y rentable para tu contenido de imágenes y texto. Está diseñado para ayudarte a detectar contenido potencialmente inseguro, incluyendo discurso de odio, violencia, material sexualmente explícito y autolesiones, entre otras capacidades.\n",
    "\n",
    "Puedes leer más sobre el servicio en este [artículo de Microsoft](https://techcommunity.microsoft.com/t5/ai-cognitive-services-blog/introducing-azure-ai-content-safety-helping-organizations-to/ba-p/3825744).\n",
    "\n",
    "**Verificación de Conocimientos #2**:\n",
    "\n",
    "Verifica tu comprensión del Servicio AI Content Safety respondiendo las siguientes preguntas:\n",
    "\n",
    "* Verdadero o Falso: La API de Moderación de Texto está diseñada para admitir más de 100 idiomas como entrada.\n",
    "* Verdadero o Falso: El Servicio AI Content Safety tiene una función para monitorear estadísticas de actividad de tu aplicación.\n",
    "* Verdadero o Falso: Azure AI Content Safety Studio y la API tienen diferentes puntuaciones de riesgo (niveles de severidad) en las categorías de daño.\n",
    "* Verdadero o Falso: Solo puedes personalizar los umbrales de severidad a través de la API.\n",
    "* Verdadero o Falso: La API siempre devuelve un nivel de severidad para las cuatro categorías de contenido."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99f9e49d",
   "metadata": {},
   "source": [
    "Para ejecutar el ejemplo, primero instala algunos paquetes y carga tus variables de entorno desde un archivo `.env`.\n",
    "\n",
    "**NOTA:** El soporte de la biblioteca openai-python para Azure OpenAI está en vista previa. Hemos especificado la versión de API Preview a continuación.\n",
    "\n",
    "`os.getenv()` asume que estás usando variables de entorno para el endpoint y la clave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220b62a1",
   "metadata": {
    "gather": {
     "logged": 1694716972271
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "assert API_KEY, \"ERROR: Azure OpenAI Key is missing\"\n",
    "openai.api_key = API_KEY\n",
    "RESOURCE_ENDPOINT = os.getenv(\"OPENAI_API_BASE\",\"\").strip()\n",
    "CHAT_MODEL = os.getenv(\"CHAT_MODEL_NAME\")\n",
    "assert RESOURCE_ENDPOINT, \"ERROR: Azure OpenAI Endpoint is missing\"\n",
    "assert \"openai.azure.com\" in RESOURCE_ENDPOINT.lower(), \"ERROR: Azure OpenAI Endpoint should be in the form: \\n\\n\\t<your unique endpoint identifier>.openai.azure.com\"\n",
    "openai.api_base = RESOURCE_ENDPOINT\n",
    "openai.api_type = os.environ['OPENAI_API_TYPE']\n",
    "CHAT_INSTRUCT_MODEL = os.getenv(\"CHAT_MODEL_NAME\")\n",
    "openai.api_version = \"2023-06-01-preview\" # API version required to test out Annotations preview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4fac5b67",
   "metadata": {},
   "source": [
    "A continuación se muestra un ejemplo de llamada a OpenAI utilizando la versión Preview que habilita las Anotaciones ([Annotations](https://learn.microsoft.com/es-mx/azure/ai-services/openai/concepts/content-filter?tabs=warning%2Cuser-prompt%2Cpython-new#annotations)). Reemplaza el prompt de entrada con diferentes textos para ver cómo cambian las anotaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9a8bad",
   "metadata": {
    "gather": {
     "logged": 1694716864019
    }
   },
   "outputs": [],
   "source": [
    "pii_prompt = \"{Example prompt where a severity level of low is detected}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55521442",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.Completion.create(\n",
    "    engine=CHAT_MODEL,\n",
    "    prompt=pii_prompt \n",
    "    # Content that is detected at severity level medium or high is filtered, \n",
    "    # while content detected at severity level low isn't filtered by the content filters.\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f314ff6",
   "metadata": {},
   "source": [
    "### 1.3 Verificación de datos PII"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44894b31",
   "metadata": {},
   "source": [
    "Los plugins son extensiones de chat diseñadas específicamente para modelos de lenguaje como ChatGPT, lo que les permite acceder a información actualizada, ejecutar cálculos o interactuar con servicios de terceros en respuesta a la solicitud de un usuario. Desbloquean una amplia gama de casos de uso potenciales y mejoran las capacidades de los modelos de lenguaje.\n",
    "\n",
    "La función mostrada a continuación, `screen_text_for_pii`, puede ser útil si deseas evitar subir documentos sensibles o privados a una base de datos de manera involuntaria.\n",
    "\n",
    "Esta característica no es infalible y puede no detectar todos los casos de información personal identificable. Utiliza esta característica con precaución y verifica su efectividad para tu caso de uso específico. Puedes leer más sobre el contexto de esta función en OpenAI [aquí](https://github.com/openai/chatgpt-retrieval-plugin/tree/main#plugins).\n",
    "\n",
    "Para otras formas de garantizar que tus datos estén seguros cuando uses OpenAI, consulta las formas de [configurar el servicio OpenAI con identidades gestionadas](https://learn.microsoft.com/es-mx/azure/cognitive-services/openai/how-to/managed-identity).\n",
    "\n",
    "Lee la función `screen_text_for_pii` en la celda siguiente para entender cómo funciona. Puedes reemplazar el texto de entrada con información relevante para tu caso de uso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae37954e",
   "metadata": {
    "gather": {
     "logged": 1694716864051
    }
   },
   "outputs": [],
   "source": [
    "def get_completion_from_messages(messages, model=CHAT_MODEL, temperature=0):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n",
    "def screen_text_for_pii(text: str) -> bool:\n",
    "    # This prompt is just an example, change it to fit your use case\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"\n",
    "            You can only respond with the word \"True\" or \"False\", where your answer indicates whether the text in the user's message contains PII.\n",
    "            Do not explain your answer, and do not use punctuation.\n",
    "            Your task is to identify whether the text extracted from your company files\n",
    "            contains sensitive PII information that should not be shared with the broader company. Here are some things to look out for:\n",
    "            - An email address that identifies a specific person in either the local-part or the domain\n",
    "            - The postal address of a private residence (must include at least a street name)\n",
    "            - The postal address of a public place (must include either a street name or business name)\n",
    "            - Notes about hiring decisions with mentioned names of candidates. The user will send a document for you to analyze.\n",
    "            \"\"\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "\n",
    "    completion = get_completion_from_messages(messages)\n",
    "    \n",
    "    if completion.startswith(\"True\"):\n",
    "        return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073cfbcb-dad4-47ba-8384-30a5d7a215b2",
   "metadata": {
    "gather": {
     "logged": 1694716864099
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Optional: test out the screening for PII using input data\n",
    "text = \"INPUT YOUR TEXT HERE\"\n",
    "screen_text_for_pii(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3cadcf88",
   "metadata": {},
   "source": [
    "## 2. Evaluar la veracidad utilizando datos de Ground Truth (Verdad Fundamental)\n",
    "\n",
    "En esta sección, nos centraremos en evaluar la veracidad en las salidas del modelo. Las alucinaciones del modelo son un problema común al usar LLMs, por lo que es importante evaluar si el modelo está generando respuestas basadas en datos en lugar de inventar información. El objetivo es mejorar la veracidad de los resultados para que tu modelo sea más consistente y fiable para la producción.\n",
    "\n",
    "Esta sección se centrará en cómo evaluar tu modelo cuando tienes acceso a datos de [Ground Truth](https://es.wikipedia.org/wiki/Verdad_fundamental). Esto nos permitirá comparar la salida del modelo con la respuesta correcta. En la siguiente sección, nos centraremos en cómo evaluar tu modelo cuando no tienes acceso a datos de Verdad Fundamental.\n",
    "\n",
    "Cuando usamos datos de Verdad Fundamental, podemos deducir una representación numérica de cuán similar es la respuesta generada a la respuesta correcta utilizando varias métricas. También tendrás la oportunidad de identificar e implementar métricas adicionales para evaluar el caso de uso en esta sección.\n",
    "\n",
    "Evaluaremos modelos utilizando conjuntos de datos de Hugging Face, así como la biblioteca [Evaluate de Hugging Face](https://huggingface.co/docs/evaluate/index).\n",
    "\n",
    "También utilizaremos LangChain, que tiene un paquete (QAEvalChain) para este propósito específico. [Lee más](https://python.langchain.com.cn/docs/guides/evaluation/question_answering) sobre cómo LangChain implementa Evaluation. Puede que hayas oído hablar de LangChain y Semantic Kernel. LangChain es un framework de código abierto de terceros que puedes usar para desarrollar aplicaciones impulsadas por modelos de lenguaje. LangChain facilita las complejidades de trabajar y construir con modelos de IA proporcionando el marco de orquestación de pipelines y utilidades de ayuda para ejecutar poderosos pipelines de múltiples modelos. También se puede integrar con Prompt Flow para escalar los flujos de trabajo de ingeniería de prompts.\n",
    "\n",
    "Al final de esta sección, podrás revisar qué enfoque (Evaluate de Hugging Face o QAEvalChain de LangChain) es preferible para futuros casos de uso."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e3ce977",
   "metadata": {},
   "source": [
    "### 2.1 Configuración\n",
    "\n",
    "Para fines de demostración, evaluaremos un sistema simple de respuesta a preguntas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9121bd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain-community langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c10054f",
   "metadata": {
    "gather": {
     "logged": 1694716864133
    }
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import AzureChatOpenAI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4cfc9b3",
   "metadata": {},
   "source": [
    "Ahora crearemos un Prompt Template (Plantilla de Prompt) que nos permitirá usar el mismo prompt con diferentes entradas. Utilizaremos [LangChain](https://docs.langchain.com/docs/), un framework de código abierto para trabajar con modelos de lenguaje.\n",
    "\n",
    "Lee más sobre LangChain Chains y cómo funcionan [aquí](https://docs.langchain.com/docs/components/chains/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abdf160",
   "metadata": {
    "gather": {
     "logged": 1694716864167
    }
   },
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template=\"Question: {question}\\nAnswer:\", input_variables=[\"question\"])\n",
    "llm = AzureChatOpenAI(deployment_name=CHAT_MODEL, temperature=0.9)\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cbea2132",
   "metadata": {},
   "source": [
    "### 2.2 Carga de datos\n",
    "\n",
    "Ahora cargamos un conjunto de datos de Hugging Face y luego lo convertimos en una lista de diccionarios para un uso más fácil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2373cf1",
   "metadata": {
    "gather": {
     "logged": 1694716864214
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"truthful_qa\", \"generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91f88b3-0b95-4814-bcd0-36527b0b49db",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Trabajemos con los primeros cinco ejemplos en el [conjunto de datos Truthful QA de Hugging Face](https://huggingface.co/datasets/truthful_qa). Estamos trabajando con la subsección \"Generation\" del conjunto de datos porque lo estamos aplicando a un sistema de generación de texto, pero observa que hay otra subsección para evaluar el rendimiento del modelo en escenarios de opción múltiple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e591ee7f",
   "metadata": {
    "gather": {
     "logged": 1694716864248
    }
   },
   "outputs": [],
   "source": [
    "num_examples = 3\n",
    "examples = list(dataset['validation'])[:num_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf71517",
   "metadata": {
    "gather": {
     "logged": 1694716864285
    }
   },
   "outputs": [],
   "source": [
    "examples[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8c3c8ef",
   "metadata": {},
   "source": [
    "### 2.3 Predicciones\n",
    "\n",
    "Ahora podemos hacer y examinar las predicciones para estas preguntas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b2849c",
   "metadata": {
    "gather": {
     "logged": 1694716864314
    }
   },
   "outputs": [],
   "source": [
    "predictions = chain.apply(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e1d71c",
   "metadata": {
    "gather": {
     "logged": 1694716864346
    }
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de420cf5",
   "metadata": {},
   "source": [
    "### 2.4 Evaluación\n",
    "Podemos ver que si intentáramos hacer una coincidencia exacta en las respuestas, no coincidirían con lo que respondió el modelo de lenguaje. Sin embargo, semánticamente el modelo de lenguaje es correcto en ambos casos. Para tener esto en cuenta, podemos usar un modelo de lenguaje para evaluar las respuestas.\n",
    "\n",
    "Debido a que estas respuestas son más complejas que las de opción múltiple, ahora podemos evaluar su precisión utilizando un modelo de lenguaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e87e11",
   "metadata": {
    "gather": {
     "logged": 1694716864379
    }
   },
   "outputs": [],
   "source": [
    "from langchain.evaluation.qa import QAEvalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc2e624",
   "metadata": {
    "gather": {
     "logged": 1694716864406
    }
   },
   "outputs": [],
   "source": [
    "# Create an Evaluation Chain using LangChain's QAEValChain\n",
    "eval_chain = QAEvalChain.from_llm(llm)\n",
    "graded_outputs = eval_chain.evaluate(examples, predictions, question_key=\"question\", answer_key=\"best_answer\", prediction_key=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10238f86",
   "metadata": {
    "gather": {
     "logged": 1694716864440
    }
   },
   "outputs": [],
   "source": [
    "graded_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d216ad-4ae9-49a9-940c-84fccac2a343",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Ahora vamos a contar el número de salidas que fueron calificadas como \"Correct\" (Correctas) o \"Incorrect\" (Incorrectas) según la evaluación de QAEvalChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e70271",
   "metadata": {
    "gather": {
     "logged": 1694715843342
    }
   },
   "outputs": [],
   "source": [
    "num_correct = sum([1 for x in graded_outputs if str(x['results']).upper().startswith('CORRECT')])\n",
    "num_incorrect = sum([1 for x in graded_outputs if str(x['results']).upper().startswith('INCORRECT')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386764e3",
   "metadata": {
    "gather": {
     "logged": 1694715843378
    }
   },
   "outputs": [],
   "source": [
    "print(num_correct, num_incorrect)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6dc737a",
   "metadata": {},
   "source": [
    "### 2.5 Comparación con otras métricas de evaluación\n",
    "\n",
    "Podemos comparar los resultados de la evaluación que obtenemos con otras métricas de evaluación comunes. Para hacer esto, carguemos algunas métricas de evaluación del paquete Evaluate de HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5f575e",
   "metadata": {
    "gather": {
     "logged": 1694715843415
    }
   },
   "outputs": [],
   "source": [
    "print(examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207be70d",
   "metadata": {
    "gather": {
     "logged": 1694715843446
    }
   },
   "outputs": [],
   "source": [
    "# Some data munging to get the examples in the right format\n",
    "for i, eg in enumerate(examples):\n",
    "    eg['id'] = str(i)\n",
    "    eg['answers'] = {\"text\": eg['correct_answers'], \"answer_start\": [0]}\n",
    "    predictions[i]['id'] = str(i)\n",
    "    predictions[i]['prediction_text'] = predictions[i]['text']\n",
    "\n",
    "for p in predictions:\n",
    "    del p['text']\n",
    "\n",
    "# references need id, answers as list with text and answer_start\n",
    "new_examples = examples.copy()\n",
    "# print(new_examples)\n",
    "for eg in new_examples:\n",
    "    del eg ['question']\n",
    "    del eg['best_answer']\n",
    "    del eg['type']\n",
    "    del eg['correct_answers']\n",
    "    del eg['category']\n",
    "    del eg['incorrect_answers']\n",
    "    del eg['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7ef7ec-d7f9-4626-b4a9-12d002737796",
   "metadata": {
    "gather": {
     "logged": 1694715843488
    }
   },
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "squad_metric = load(\"squad\")\n",
    "results = squad_metric.compute(\n",
    "    references=new_examples,\n",
    "    predictions=predictions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2ab1dc",
   "metadata": {
    "gather": {
     "logged": 1694715843520
    }
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4df686a3",
   "metadata": {},
   "source": [
    "#### (Opcional) Tarea del Estudiante\n",
    "\n",
    "Ahora agrega dos métricas adicionales para evaluar el modelo utilizando la biblioteca Evaluate de Hugging Face. Una de esas métricas podría ser la métrica BERT_score.\n",
    "\n",
    "Recursos de referencia:\n",
    "\n",
    "* [Biblioteca Evaluate de Hugging Face en GitHub](https://github.com/huggingface/evaluate) \n",
    "* [Documentación de la Biblioteca Evaluate](https://huggingface.co/docs/transformers/tasks/translation#evaluate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2424f5f7",
   "metadata": {
    "gather": {
     "logged": 1694715843567
    }
   },
   "outputs": [],
   "source": [
    "### TAREA DEL ESTUDIANTE ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f88f1a76",
   "metadata": {},
   "source": [
    "## 3. Evaluación de Modelos para determinar su Veracidad mediante GPT sin Conjuntos de Datos de Verdad Fundamental"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "767bceab",
   "metadata": {},
   "source": [
    "No siempre tendrás datos de Ground Truth (Verdad Fundamental) disponibles para evaluar tu modelo. Afortunadamente, GPT hace un muy buen trabajo al generar datos de Verdad Fundamental a partir de tu conjunto de datos original.\n",
    "\n",
    "La investigación ha demostrado que los LLMs como GPT-3 y ChatGPT son buenos para evaluar la inconsistencia del texto. Basándonos en estos hallazgos, los modelos pueden usarse para evaluar la veracidad de las oraciones mediante prompts a GPT. Evaluemos la precisión de GPT a través de una técnica en la que GPT se evalúa a sí mismo.\n",
    "\n",
    "En esta sección, evaluaremos el modelo en el que trabajaste en el desafío anterior aplicado al conjunto de datos CNN Dailymail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7e21f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain, QAGenerationChain\n",
    "from langchain.requests import Requests\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a19c79e",
   "metadata": {},
   "source": [
    "### 3.1. Crear un Conjunto de Datos de Verdad Fundamental en Datos Personalizados\n",
    "Comencemos utilizando GPT para crear un conjunto de datos de pares de preguntas y respuestas como nuestros datos de \"verdad fundamental\" a partir del conjunto de datos CNN Dailymail del desafío anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9681f823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the provided CNN file, the path of which may change based on folder structure\n",
    "CNN_FILE_PATH = \"../data/structured/cnn_dailymail_data.csv\"\n",
    "\n",
    "# Optional: limit to 11 samples for simple scope to avoid RateLimitErrors\n",
    "# You are welcome to change `num_samples` or delete it to run this example on\n",
    "# the entire dataset but doing so may take 1+ hour\n",
    "num_samples = 11\n",
    "df = pd.read_csv(CNN_FILE_PATH)[:num_samples]\n",
    "df.drop([4,9], axis=0, inplace=True)\n",
    "df = df.drop(columns=[\"highlights\"])\n",
    "pd.set_option('display.max_colwidth', None)  # Show all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9ad597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the data\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b447a9b6-3f2d-4bf9-84a5-4f269c78b45a",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Es hora de hacer un poco de limpieza de datos para garantizar la consistencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6599f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the column \"article\" to a list of dictionaries\n",
    "df_copy = df.copy().rename(columns={\"article\": \"text\"})\n",
    "df_copy = df_copy.drop(columns=[\"id\"])\n",
    "df_dict = df_copy.to_dict(\"records\")\n",
    "\n",
    "print(df_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf8a1f8-369f-4fb4-909c-600e1c5102d3",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Hemos generado un par de preguntas y respuestas para cada artículo. Esto nos ayudará a evaluar el rendimiento de GPT en cuanto a la calidad de sus respuestas a las preguntas de prueba. Las respuestas en cada par se consideran nuestros datos de verdad fundamental y la respuesta ideal.\n",
    "\n",
    "Creamos estos pares utilizando [QAGenerationChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.qa_generation.base.QAGenerationChain.html#) de Langchain. Consulta el [código fuente](https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/chains/qa_generation) para ver cómo se generan los pares de preguntas y respuestas a través de QAGenerationChain. ¡La implementación puede sorprenderte!\n",
    "\n",
    "En el proceso, eliminamos los artículos que activaron el filtro de contenido de OpenAI.\n",
    "\n",
    "A continuación, vamos a cargar el conjunto de datos de preguntas y respuestas proporcionado para la evaluación posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af36819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureOpenAI(deployment_name=CHAT_MODEL, temperature=0, max_tokens=1000)\n",
    "chain = QAGenerationChain.from_llm(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc5ba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cnn_qa_set.json\n",
    "cnn_qa_set_filepath = '../data/structured/cnn_qa_set.json'\n",
    "with open(cnn_qa_set_filepath, 'r') as file:\n",
    "    qa_set = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b597401-c969-454f-a7ee-6a91195239f7",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "qa_set[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd04f598",
   "metadata": {},
   "source": [
    "Ahora tenemos las preguntas y las respuestas de Verdad Fundamental. ¡Probemos la solución de GPT + AI Search que implementaste en el último desafío! Vamos a comparar las diferencias entre `truth_answers` (respuestas proporcionadas) y `prompt_answers` (desempeño del modelo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06d9ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [(set[\"question\"] for set in qa_set)]\n",
    "truth_answers = [(set[\"answers\"] for set in qa_set)]\n",
    "prompt_answers = list()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c9127ee",
   "metadata": {},
   "source": [
    "### 3.2 Instanciar el Índice de AI Search\n",
    "\n",
    "Estamos usando el Índice que creaste en el último desafío para recuperar documentos que sean relevantes para cualquier consulta de usuario de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c97e90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, requests, sys, re\n",
    "import requests\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient \n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    SemanticConfiguration,\n",
    "    PrioritizedFields,\n",
    "    SemanticField,\n",
    "    SemanticSettings\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "from openai.embeddings_utils import get_embedding, cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38c0644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an SDK client\n",
    "service_endpoint = os.getenv(\"AZURE_COGNITIVE_SEARCH_ENDPOINT\")   \n",
    "key = os.getenv(\"AZURE_COGNITIVE_SEARCH_KEY\")\n",
    "credential = AzureKeyCredential(key)\n",
    "index_name = os.getenv(\"AZURE_COGNITIVE_SEARCH_INDEX_NAME\")\n",
    "\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=service_endpoint, credential=credential)\n",
    "search_client = SearchClient(endpoint=service_endpoint, index_name=index_name, credential=credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0501d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pandas dataframe with columns from qa_set\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df = pd.DataFrame(qa_set)\n",
    "df = df.rename(columns={\"answer\": \"truth_answer\"})\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e794a118-188b-4e70-b0f0-3376146d077c",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Vamos a recuperar los artículos relevantes para cada pregunta en nuestro dataframe `qa_set`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f48c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the articles for the search terms\n",
    "# Optional: change `num_docs` to change how many relevant ranked documents the Search index should return\n",
    "num_docs=1\n",
    "for i, row in df.iterrows():\n",
    "    search_term = row['question']\n",
    "    results = search_client.search(search_text=search_term, include_total_count=num_docs)\n",
    "    df.loc[i, \"context\"] = next(results)['article']\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c172dfb-a2fa-48b2-b723-96d6d521d7e8",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Usando una plantilla de prompt (prompt template), podemos introducir preguntas a GPT utilizando la información de los documentos recuperados.\n",
    "\n",
    "Observa qué modelo estamos usando ahora para generar respuestas. ¿A qué se debe esto? ¿Qué sucede si utilizas el modelo de chat que usamos anteriormente?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31713b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Ask the model using the embeddings from Challenges 3 and 4 to answer the questions\n",
    "template = \"\"\"You are a search assistant trying to answer the following question. Use only the context given. Your answer should only be one sentence.\n",
    "\n",
    "    > Question: {question}\n",
    "    \n",
    "    > Context: {context}\"\"\"\n",
    "\n",
    "# Create a prompt template\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\", \"context\"])\n",
    "llm = AzureOpenAI(deployment_name=CHAT_INSTRUCT_MODEL, temperature=0)\n",
    "search_chain = LLMChain(llm=llm, prompt=prompt, verbose=False)\n",
    "\n",
    "prompt_answers = []\n",
    "for question, context in list(zip(df.question, df.context)):\n",
    "    response = search_chain.run(question=question, context=context)\n",
    "    prompt_answers.append(response.replace('\\n',''))\n",
    "df['prompt_answer'] = prompt_answers   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252b880b",
   "metadata": {},
   "source": [
    "Examina las tres primeras respuestas del modelo basadas en los artículos. ¿Cómo podrías utilizar técnicas de Prompt Engineering para refinar las respuestas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569b27de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prompt_answer'].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e340641-3ffd-4ea0-a35d-5a684b17d40d",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Después de generar respuestas a nuestras preguntas de prueba, podemos usar GPT (puede ser otro modelo si lo prefieres, como GPT-4) para evaluar la exactitud de nuestras respuestas de Verdad Fundamental utilizando una rúbrica.\n",
    "\n",
    "Observa cómo el prompt está utilizando técnicas que aprendiste en los Desafíos 1 y 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0583718",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_template = \"\"\"You are trying to answer the following question from the context provided:\n",
    "\n",
    "> Question: {question}\n",
    "\n",
    "The correct answer is:\n",
    "\n",
    "> Query: {truth_answer}\n",
    "\n",
    "Is the following predicted query semantically the same (eg likely to produce the same answer)?\n",
    "\n",
    "> Predicted Query: {prompt_answer}\n",
    "\n",
    "Please give the Predicted Query a grade of either an A, B, C, D, or F, along with an explanation of why. End the evaluation with 'Final Grade: <the letter>'\n",
    "\n",
    "> Explanation: Let's think step by step.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba357c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompt = PromptTemplate(template=eval_template, input_variables=[\"question\", \"truth_answer\", \"prompt_answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8062ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new LLM Chain to submit the prompt we created\n",
    "eval_chain = LLMChain(llm=llm, prompt=eval_prompt, verbose=False)\n",
    "\n",
    "# Submit the prompt using our dataset\n",
    "eval_results = []\n",
    "for question, truth_answer, prompt_answer in list(zip(df.question, df.truth_answer, df.prompt_answer)):\n",
    "    eval_output = eval_chain.run(\n",
    "        question=question,\n",
    "        truth_answer=truth_answer,\n",
    "        prompt_answer=prompt_answer,\n",
    "    )\n",
    "    eval_results.append(eval_output)\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dabe6a-659a-44ca-bc82-ac8793f713ef",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Ahora vamos a analizar los resultados de la rúbrica para cuantificarlos y resumirlos en conjunto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8449fb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "from collections import defaultdict\n",
    "\n",
    "# Parse the evaluation chain responses into a rubric\n",
    "def parse_eval_results(results: List[str]) -> List[float]:\n",
    "    rubric = {\n",
    "        \"A\": 1.0,\n",
    "        \"B\": 0.75,\n",
    "        \"C\": 0.5,\n",
    "        \"D\": 0.25,\n",
    "        \"F\": 0\n",
    "    }\n",
    "    return [rubric[re.search(r'Final Grade: (\\w+)', res).group(1)] for res in results]\n",
    "\n",
    "scores = defaultdict(list)\n",
    "parsed_results = parse_eval_results(eval_results)\n",
    "\n",
    "# Collect the scores for a final evaluation table\n",
    "scores['request_synthesizer'].extend(parsed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2090296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reusing the rubric from above, parse the evaluation chain responses\n",
    "parsed_eval_results = parse_eval_results(eval_results)\n",
    "# Collect the scores for a final evaluation table\n",
    "scores['result_synthesizer'].extend(parsed_eval_results)\n",
    "\n",
    "# Print out Score statistics for the evaluation session\n",
    "header = \"{:<20}\\t{:<10}\\t{:<10}\\t{:<10}\".format(\"Metric\", \"Min\", \"Mean\", \"Max\")\n",
    "print(header)\n",
    "for metric, metric_scores in scores.items():\n",
    "    mean_scores = sum(metric_scores) / len(metric_scores) if len(metric_scores) > 0 else float('nan')\n",
    "    row = \"{:<20}\\t{:<10.2f}\\t{:<10.2f}\\t{:<10.2f}\".format(metric, min(metric_scores), mean_scores, max(metric_scores))\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a055d128-672d-4bd7-83cf-544ad9b6a803",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "¡Ahí lo tienes! Ahora podemos revisar los resultados de la evaluación del modelo en conjunto con Azure AI Search del último desafío. Puedes realizar un análisis similar en tu caso de uso y datos personalizados."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b743a96",
   "metadata": {},
   "source": [
    "## Conclusión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461ee50f-c2c4-4a80-bc0e-68fad17ee380",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "En este desafío, cubrimos los principios de la IA Responsable, particularmente al trabajar con OpenAI, y cómo evaluar el rendimiento de una implementación de modelo utilizando datos de Verdad Fundamental.\n",
    "\n",
    "Te presentamos varias herramientas y servicios, algunos de Azure y otros que son de código abierto. Puedes referirte a ellos para tus propios proyectos y decidir cuál funciona mejor para tus escenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718b928c-6041-4aba-9d65-23b493fbf84c",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**Respuestas a la Verificación de Conocimientos #1:**:\n",
    "* Verdadero\n",
    "* Falso - se devolverá si no se considera inapropiado\n",
    "* Falso - tu solicitud aún se completará sin filtrado de contenido. Puedes ver si no se aplicó buscando un mensaje de error en el objeto `content_filter_result`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55a9618",
   "metadata": {},
   "source": [
    "**Respuestas a la Verificación de Conocimientos #2:**:\n",
    "\n",
    "* Falso: El servicio se entrenó en más de 100 idiomas, pero está diseñado para admitir solo unos pocos.\n",
    "* Verdadero: Content Safety tiene una página de monitoreo para ayudarte a rastrear el rendimiento y las tendencias de tu API de moderación e informar tu estrategia de moderación de contenido.\n",
    "* Verdadero: El Studio utiliza cuatro niveles de riesgo, mientras que la API puntúa el riesgo en una escala del 0 al 6.\n",
    "* Falso: También puedes personalizar los umbrales de severidad en el Studio.\n",
    "* Falso: Puedes especificar en qué categorías deseas evaluar tu texto en la API utilizando el parámetro `categories`."
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "microsoft": {
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
