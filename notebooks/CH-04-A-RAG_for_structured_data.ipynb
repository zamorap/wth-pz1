{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18002aa6-39a1-4e71-8749-324e2f615f31",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Reto 04-A - Generación Aumentada con Recuperación (RAG) para Datos Estructurados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4ca1751",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "En este notebook, exploraremos la aplicación práctica de RAG con un tipo de datos más manejable, es decir, datos estructurados como datos relacionales o datos de texto almacenados en archivos CSV. El objetivo principal es introducir un caso de uso específico que demuestre la utilización de Azure AI Search para extraer documentos relevantes y el poder de ChatGPT para abordar las partes relevantes del documento, proporcionando resúmenes concisos basados en los prompts del usuario. Tiene como objetivo mostrar cómo se pueden adaptar las capacidades de ChatGPT de Azure OpenAI a tus necesidades de resumen, al mismo tiempo que te guía a través de la configuración y evaluación de los resultados del resumen. Este método se puede personalizar para adaptarse a varios casos de uso de resumen y aplicarse a diversos conjuntos de datos.\n",
    "\n",
    "Tus objetivos para este desafío son leer este notebook, ejecutar cada bloque de código, observar los resultados y luego poder responder las preguntas planteadas en la guía del estudiante."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f2d0025-3952-481b-9615-cfe5ee198f66",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Caso de Uso\n",
    "\n",
    "Este caso de uso consta de tres secciones:\n",
    "- Búsqueda de Documentos: El proceso de extraer documentos relevantes en función de la consulta de un corpus de documentos.\n",
    "- Búsqueda de Zona de Documentos: El proceso de encontrar la parte relevante del documento extraído de la búsqueda de documentos.\n",
    "- Tareas de IA posteriores, como Responder a Preguntas (también conocidas como Resumen de Texto): el resumen de texto es el proceso de crear resúmenes a partir de grandes volúmenes de datos manteniendo elementos informativos significativos y valor de contenido.\n",
    "\n",
    "Este caso de uso puede ser útil para ayudar a expertos en la materia a encontrar información relevante en un gran corpus de documentos.\n",
    "\n",
    "**Ejemplo:** En el proceso de descubrimiento de medicamentos, los científicos de la industria farmacéutica leen un corpus de documentos para encontrar información específica relacionada con conceptos, resultados de experimentos, etc. Este caso de uso les permite hacer preguntas al corpus de documentos y la solución devolverá la respuesta sucinta. En consecuencia, se acelera el proceso de descubrimiento de medicamentos.\n",
    " \n",
    "Beneficios de la solución:\n",
    "1. Acorta el tiempo de lectura.\n",
    "2. Mejora la efectividad de la búsqueda de información.\n",
    "3. Elimina el sesgo de las técnicas de resumen humanas.\n",
    "4. Aumenta la capacidad para que los humanos se enfoquen en análisis más profundos.\n",
    "\n",
    "La necesidad de resumen de documentos puede aplicarse a cualquier tema (legal, financiero, periodístico, médico, académico, etc.) que requiera un resumen de documentos largos. El tema en el que se centra este notebook es periodístico: recorreremos artículos de noticias."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85743c37-40f6-493f-9eaa-e9c4857ba8eb",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Conjunto de Datos de CNN y Daily Mail\n",
    "Para este tutorial, utilizaremos el conjunto de datos de CNN/Daily Mail. Se trata de un conjunto de datos comúnmente utilizado para tareas de resumen de texto y respuesta a preguntas. Se generaron resúmenes abstractivos humanos a partir de historias de noticias en los sitios web de CNN y Daily Mail.\n",
    "\n",
    "### Descripción de los Datos\n",
    "El esquema relevante para nuestro trabajo de hoy consiste en:\n",
    "\n",
    "- `id`: una cadena que contiene el hash SHA1 con formato hexadecimal de la URL de donde se recuperó la noticia.\n",
    "- `article`: una cadena de caracteres que contiene el cuerpo del artículo de noticias.\n",
    "- `highlights`: una cadena de caracteres que contiene lo más destacado del artículo tal como lo escribió el autor del artículo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bd738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Azure Cognitive Search, OpenAI, and other python modules\n",
    "\n",
    "import os, json, requests, sys, re\n",
    "import requests\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient \n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    SemanticConfiguration,\n",
    "    PrioritizedFields,\n",
    "    SemanticField,\n",
    "    SemanticSettings\n",
    ")\n",
    "\n",
    "\n",
    "import openai\n",
    "import numpy as np\n",
    "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4cc4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is secure and recommended way to load OpenAI resource credentials and deployment names\n",
    "\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "openai.api_base = os.environ['OPENAI_API_BASE']\n",
    "openai.api_type = os.environ['OPENAI_API_TYPE']\n",
    "openai.api_version = os.environ['OPENAI_API_VERSION']\n",
    "\n",
    "chat_model = os.environ['CHAT_MODEL_NAME']\n",
    "embedding_model=os.environ['EMBEDDING_MODEL_NAME']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20c44e3",
   "metadata": {},
   "source": [
    "**NOTA:** La ruta en la celda de código a continuación hace referencia al archivo `cnn_dailymail.csv` en la carpeta `/data/structured/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9019bf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the CNN dailymail dataset in pandas dataframe\n",
    "df = pd.read_csv('../data/structured/cnn_dailymail_data.csv') #path to CNN daily mail dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eff67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Cognitive Search Index client\n",
    "service_endpoint = os.getenv(\"AZURE_COGNITIVE_SEARCH_ENDPOINT\")   \n",
    "key = os.getenv(\"AZURE_COGNITIVE_SEARCH_KEY\")\n",
    "credential = AzureKeyCredential(key)\n",
    "\n",
    "index_name = os.getenv(\"AZURE_COGNITIVE_SEARCH_INDEX_NAME\")\n",
    "\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=service_endpoint, credential=credential)\n",
    "index_client"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b30a67db",
   "metadata": {},
   "source": [
    "### Definir Campos del Índice y Crear una Configuración Semántica\n",
    "\n",
    "Una *configuración semántica* especifica cómo se utilizan los campos en la clasificación semántica. Da a los modelos subyacentes pistas sobre qué campos de índice son más importantes para la clasificación semántica, los subtítulos, los destacados y las respuestas.\n",
    "\n",
    "Puedes agregar o actualizar una configuración semántica en cualquier momento sin reconstruir tu índice. Cuando emites una consulta, añadirás la configuración semántica (una por consulta) que especifica qué configuración semántica usar para la consulta.\n",
    "\n",
    "Revisa las propiedades que necesitarás especificar. Una configuración semántica tiene un nombre y al menos una de cada una de las siguientes propiedades:\n",
    "\n",
    "* Campo de título: Un campo de título debe ser una descripción concisa del documento, idealmente una cadena de menos de 25 palabras. Este campo podría ser el título del documento, el nombre del producto o el elemento en tu índice de búsqueda. Si no tienes un título en tu índice de búsqueda, deja este campo en blanco.\n",
    "* Campos de contenido: Los campos de contenido deben contener texto en formato de lenguaje natural. Ejemplos comunes de contenido son el cuerpo de un documento, la descripción de un producto u otro texto en forma libre.\n",
    "* Campos de palabras clave: Los campos de palabras clave deben ser una lista de palabras clave, como las etiquetas en un documento, o un término descriptivo, como la categoría de un artículo.\n",
    "\n",
    "Solo puedes especificar un campo de título, pero puedes especificar tantos campos de contenido y palabras clave como desees. Para los campos de contenido y palabras clave, enumera los campos en orden de prioridad porque los campos de menor prioridad pueden ser truncados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a325dd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "    SearchableField(name=\"highlights\", type=SearchFieldDataType.String,\n",
    "                searchable=True, retrievable=True),\n",
    "    SearchableField(name=\"article\", type=SearchFieldDataType.String,\n",
    "                filterable=True, searchable=True, retrievable=True),\n",
    "]\n",
    "\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"my-semantic-config\",\n",
    "    prioritized_fields=PrioritizedFields(\n",
    "        #title_field=SemanticField(field_name=\"\"), # title field is not present in the dataset. We can use OpenAI to generate title\n",
    "        #prioritized_keywords_fields=[SemanticField(field_name=\"\")], # keywords are not present in the dataset. We can use OpenAI to generate keywords\n",
    "        prioritized_content_fields=[SemanticField(field_name=\"article\"), SemanticField(field_name=\"highlights\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# Create the semantic settings with the configuration\n",
    "semantic_settings = SemanticSettings(configurations=[semantic_config])\n",
    "\n",
    "# Create the search index with the semantic settings\n",
    "index = SearchIndex(name=index_name, fields=fields, semantic_settings=semantic_settings)\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(f' {result.name} created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a028ea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataframe to a list of dictionaries\n",
    "documents = df.to_dict('records')\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b3fe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9022d69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_client = SearchClient(endpoint=service_endpoint, index_name=index_name, credential=credential)\n",
    "result = search_client.upload_documents(documents)  \n",
    "print(f\"Uploaded and Indexed {len(result)} documents\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc09a779-e3cd-485f-ae3a-297491d993b0",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Sección 1: Aprovechar la Búsqueda Cognitiva para extraer artículos relevantes basados en la consulta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32689db7-4337-42d9-b8f9-4cbd9d98a850",
   "metadata": {
    "gather": {
     "logged": 1675138710195
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Extracting relevant article based on the query. eg: Clinton Democratic Nomination\n",
    "results = search_client.search(search_text=\"Clinton Democratic nomination\", include_total_count=True)\n",
    "document = next(results)['article']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9681f2-2448-4e6d-8174-5fb5ff61d5db",
   "metadata": {
    "gather": {
     "logged": 1675139624461
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02375dcd-514e-4203-951e-729b3de07570",
   "metadata": {
    "gather": {
     "logged": 1675139635796
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#length of article extracted from Azure Cognitive search\n",
    "len(document) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30b4b060-1dca-468c-a1f5-ac1b9e5d4878",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Sección 2: Búsqueda de Zona de Documentos\n",
    "### Zona de Documentos: API de Embeddings de Azure OpenAI\n",
    "Ahora que nos hemos enfocado en un solo documento de nuestra base de conocimientos usando Azure AI Search, podemos profundizar en dicho documento único para refinar nuestra consulta inicial a una sección más específica o \"zona\" del artículo.\n",
    "\n",
    "Para hacer esto, utilizaremos la API de Embeddings de Azure OpenAI.\n",
    "\n",
    "### **Descripción general de los Embeddings**\n",
    "Un embedding es un formato especial de representación de datos que puede ser fácilmente utilizado por modelos y algoritmos de machine learning. El embedding es una representación densa en información del significado semántico de un fragmento de texto. Cada embedding es un vector de números de punto flotante, de modo que la distancia entre dos embeddings en el espacio vectorial está correlacionada con la similitud semántica entre dos entradas en el formato original. Por ejemplo, si dos textos son similares, entonces sus representaciones vectoriales también deberían ser similares.\n",
    "\n",
    "Diferentes modelos de embedding de Azure OpenAI están específicamente creados para ser buenos en una tarea particular. Los embeddings de similitud son buenos para capturar la similitud semántica entre dos o más fragmentos de texto. Los embeddings de búsqueda de texto ayudan a medir qué documentos largos son relevantes para una consulta corta. Los embeddings de búsqueda de código son útiles para vectorizar fragmentos de código y consultas de búsqueda en lenguaje natural.\n",
    "\n",
    "Los embeddings facilitan el machine learning en grandes entradas que representan palabras al capturar las similitudes semánticas en un espacio vectorial. Por lo tanto, podemos usar embeddings para determinar si dos fragmentos de texto están semánticamente relacionados o son similares, y de manera inherente proporcionar una puntuación para evaluar la similitud.\n",
    "\n",
    "### **Similitud de Coseno**\n",
    "Un enfoque utilizado anteriormente para localizar documentos similares se basaba en contar el número máximo de palabras comunes entre documentos. Esto es defectuoso ya que, a medida que aumenta el tamaño del documento, aumenta la superposición de palabras comunes incluso si los temas difieren. Por lo tanto, la similitud del coseno es un mejor enfoque.\n",
    "\n",
    "Matemáticamente, la similitud del coseno mide el coseno del ángulo entre dos vectores proyectados en un espacio multidimensional. Esto es beneficioso porque si dos documentos están muy separados por la distancia euclidiana debido al tamaño, aún podrían tener un ángulo más pequeño entre ellos y, por lo tanto, una mayor similitud del coseno.\n",
    "\n",
    "Los embeddings de Azure OpenAI se basan en la similitud del coseno para calcular la similitud entre documentos y una consulta."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1cf78f21-368a-4314-ab59-f5be527e4b08",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Configuración del servicio de Azure OpenAI y uso de modelos implementados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c178e3be",
   "metadata": {},
   "source": [
    "### **Chunking**\n",
    "\n",
    "Comencemos con la segmentación (chunking). ¿Por qué es importante chunking al trabajar con LLMs?\n",
    "\n",
    "Chunking ayuda a superar los desafíos asociados con el procesamiento de secuencias largas y garantiza un rendimiento óptimo al trabajar con LLMs.\n",
    "\n",
    "**Mitigación de Limitaciones de Tokens:** Los LLMs tienen un límite máximo de tokens para cada secuencia de entrada. Si un documento o entrada excede este límite, necesita dividirse en fragmentos que se ajusten a las restricciones de tokens. Chunking permite que el LLM maneje documentos largos o entradas dividiéndolos en múltiples fragmentos que caen dentro del límite de tokens. Esto asegura que el modelo pueda procesar efectivamente todo el contenido mientras se adhiere a las restricciones de tokens.\n",
    "\n",
    "**Eficiencia de Memoria y Computacional:** Los LLMs son computacionalmente costosos y requieren recursos de memoria sustanciales para procesar secuencias largas de texto. Chunking implica descomponer documentos largos o entradas en fragmentos más pequeños y manejables, permitiendo que el LLM los procese eficientemente dentro de sus limitaciones de memoria. Al dividir la entrada en partes más pequeñas, chunking ayuda a evitar errores de memoria o degradación del rendimiento que pueden ocurrir al procesar secuencias largas.\n",
    "\n",
    "**Coherencia Contextual:** Chunking ayuda a mantener la coherencia contextual en las salidas generadas. En lugar de tratar toda la entrada como una sola secuencia, dividirla en fragmentos más pequeños permite que el modelo capture el contexto local de manera más efectiva. Esto mejora la comprensión del modelo de las relaciones y dependencias dentro del texto, lo que lleva a respuestas generadas más coherentes y significativas.\n",
    "\n",
    "**Paralelismo Mejorado:** Chunking permite el procesamiento paralelo, lo cual es esencial para optimizar el rendimiento de los LLMs. Al dividir la entrada en fragmentos, se pueden procesar múltiples fragmentos simultáneamente, aprovechando las capacidades de computación paralela. Esto conduce a tiempos de inferencia más rápidos y mejora la eficiencia general al trabajar con LLMs.\n",
    "\n",
    "Utilizaremos un splitter básico para este notebook. Sin embargo, es importante tener en cuenta que existen splitters más avanzados disponibles, que pueden adaptarse mejor a tu caso de uso específico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b043fbb4",
   "metadata": {
    "gather": {
     "logged": 1675138711079
    }
   },
   "outputs": [],
   "source": [
    "#Defining helper functions\n",
    "#Splits text after sentences ending in a period. Combines n sentences per chunk.\n",
    "def splitter(n, s):\n",
    "    pieces = s.split(\". \")\n",
    "    list_out = [\" \".join(pieces[i:i+n]) for i in range(0, len(pieces), n)]\n",
    "    return list_out\n",
    "\n",
    "# Perform light data cleaning (removing redudant whitespace and cleaning up punctuation)\n",
    "def normalize_text(s, sep_token = \" \\n \"):\n",
    "    s = re.sub(r'\\s+',  ' ', s).strip()\n",
    "    s = re.sub(r\". ,\",\"\",s)\n",
    "    # remove all instances of multiple spaces\n",
    "    s = s.replace(\"..\",\".\")\n",
    "    s = s.replace(\". .\",\".\")\n",
    "    s = s.replace(\"\\n\", \"\")\n",
    "    s = s.strip()    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56354758-427f-4af9-94b9-96a25946e9a5",
   "metadata": {
    "gather": {
     "logged": 1675138711316
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "document_chunks = splitter(10, normalize_text(document)) #splitting extracted document into chunks of 10 sentences\n",
    "document_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b3c83f-deca-493b-aa41-12b89f24feff",
   "metadata": {
    "gather": {
     "logged": 1675138711716
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Handling Rate Limits\n",
    "\n",
    "from openai.error import RateLimitError\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "def get_embedding(text: str, engine: str = \"text-embedding-ada-002\"):\n",
    "    count=0\n",
    "    while True:\n",
    "        try:\n",
    "            embedding = openai.Embedding().create(input=[text], engine=engine)[\"data\"][0][\"embedding\"]\n",
    "            break;\n",
    "        except RateLimitError:\n",
    "            count+=1\n",
    "            #print(f'RateLimitError Count: {count}')\n",
    "            sleep(2)            \n",
    "    return np.array(embedding).astype(np.float32)\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-35-turbo\", temperature=0): \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de635ba5-7cf1-4d4f-8598-73619fc9c7ef",
   "metadata": {
    "gather": {
     "logged": 1675138711984
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "embed_df = pd.DataFrame(document_chunks, columns = [\"chunks\"]) #datframe with document chunks\n",
    "\n",
    "#Create an embedding vector for each chunk that will capture the semantic meaning and overall topic of that chunk\n",
    "embed_df['embeddings'] = embed_df[\"chunks\"].apply(lambda x : get_embedding(x, engine = embedding_model))\n",
    "\n",
    "embed_df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc7adb8-93dd-4dfd-995a-8df893a98d99",
   "metadata": {
    "gather": {
     "logged": 1675138712417
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# search through the document for a text segment most similar to the query\n",
    "# display top two most similar chunks based on cosine similarity\n",
    "def search_docs(df, user_query, top_n=3):\n",
    "    embedding = get_embedding(\n",
    "        user_query,\n",
    "        engine=embedding_model,\n",
    "    )\n",
    "    df[\"similarities\"] = df['embeddings'].apply(lambda x: cosine_similarity(x, embedding))\n",
    "\n",
    "    res = (\n",
    "        df.sort_values(\"similarities\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "        .head(top_n)\n",
    "    )\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8511f3a-198f-4e8f-8a5d-cb74456826fa",
   "metadata": {
    "gather": {
     "logged": 1675138712650
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "document_specific_query = \"trouble so far in clinton campaign\" \n",
    "res = search_docs(embed_df, document_specific_query, top_n=2) #finding top 2 results based on similarity \n",
    "res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eabac33e-5a98-49f0-8fd6-2750bcf79bb1",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Sección 3: Resumen de Texto\n",
    "\n",
    "Esta sección trata del flujo de principio a fin del uso de los modelos GPT-3 y ChatGPT para tareas de resumen. El modelo utilizado por el servicio Azure OpenAI es una llamada de completado generativo que utiliza instrucciones en lenguaje natural para identificar la tarea solicitada y la habilidad requerida, también conocido como Prompt Engineering. Usando este enfoque, la primera parte del prompt incluye instrucciones en lenguaje natural y/o ejemplos de la tarea específica deseada. Luego, el modelo completa la tarea prediciendo el próximo texto más probable. Esta técnica se conoce como aprendizaje \"en contexto\".\n",
    "\n",
    "Hay tres enfoques principales para el aprendizaje en contexto: Zero-shot, Few-shot y Fine-tuning. Estos enfoques varían según la cantidad de datos específicos de la tarea que se proporcionan al modelo:\n",
    "\n",
    "**Zero-shot (cero disparos):** En este caso, no se proporcionan ejemplos al modelo y solo se proporciona la solicitud de la tarea.\n",
    "\n",
    "**Few-shot (pocos disparos):** En este caso, un usuario incluye varios ejemplos en el prompt de llamada que demuestran el formato y el contenido de la respuesta esperada.\n",
    "\n",
    "**Fine-Tuning (ajuste preciso):** Fine Tuning te permite adaptar los modelos a tus conjuntos de datos personales. Este paso de personalización te permitirá obtener más del servicio proporcionando:\n",
    "-   Con muchos datos (al menos 500 y más), se utilizan técnicas de optimización tradicionales con Back Propagation para reajustar los pesos del modelo - esto permite obtener resultados de mayor calidad que las técnicas simples de zero-shot o few-shot.\n",
    "-   Un modelo personalizado mejora el enfoque de few-shot learning (aprendizaje de pocos disparos) al entrenar los pesos del modelo en tus prompts y estructuras específicas. Esto te permite lograr mejores resultados en una mayor cantidad de tareas sin necesidad de proporcionar ejemplos en el prompt. El resultado es menos texto enviado y menos tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8e47a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Designing a prompt that will show and tell GPT-3 how to proceed. \n",
    "+ Providing an instruction to summarize the text about the general topic (prefix)\n",
    "+ Providing quality data for the chunks to summarize and specifically mentioning they are the text provided (context + context primer)\n",
    "+ Providing a space for GPT-3 to fill in the summary to follow the format (suffix)\n",
    "'''\n",
    "\n",
    "# result_1 corresponding to the top chunk from Section 2. result_2 corresponding to the second to top chunk from section 2. \n",
    "# change index for desired chunk\n",
    "result_1 = res.chunks[0]\n",
    "result_2 = res.chunks[1]\n",
    "prompt_i = 'Summarize the content about the Clinton campaign given the text provided.\\n\\nText:\\n'+\" \".join([normalize_text(result_1)])+ '\\n\\nText:\\n'+ \" \".join([normalize_text(result_2)])+'\\n\\nSummary:\\n'\n",
    "print(prompt_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec85c16-daec-4eb3-aa33-bed7c20774b6",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "get_completion(prompt_i, model=chat_model) # default temperature is set to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1449c949-1a01-4f20-bebb-e7674ac6de43",
   "metadata": {
    "gather": {
     "logged": 1675138714150
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# bumping up the temperature to 0.5 to increase the randomness of the model's output\n",
    "get_completion(prompt_i, model=chat_model, temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609f8afe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "2139c70ac98f3202d028164a545621647e07f47fd6f5d8ac55cf952bf7c15ed1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
